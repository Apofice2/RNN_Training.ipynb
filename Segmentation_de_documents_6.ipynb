{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apofice2/RNN_Training.ipynb/blob/main/Segmentation_de_documents_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG_VLDoUmGtx"
      },
      "source": [
        "# Chapitre 1 : Prétraitement de fichiers texte\n",
        "\n",
        "## 1. Segmentation et Tokenization\n",
        "\n",
        "### 1.1 From scratch ou presque\n",
        "\n",
        "On commence par la méthode la plus directe de segmentation. Elle s'appuie sur la méthode standard $\\texttt{str.split()}$ de la classe $\\texttt{str}$\n",
        "\n",
        "->https://docs.python.org/fr/3.6/library/stdtypes.html?highlight=split#str.split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "HOICUPMymGt0"
      },
      "outputs": [],
      "source": [
        "sentence='''Plato was a famous greek philosopher whose dialogue Cratylus mainly deals with the correctness of names. \\n '''\n",
        "print(sentence)\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CX0u1mLmGt0"
      },
      "source": [
        "C'est un résultat satisfaisant en première approximation. A noter cependant 'philosopher.' et 'names.' Certains jetons associent segmentation et marques de ponctuation. Ce genre de \"détails\" rend délicate la conception de Tokenizer versatiles.\n",
        "\n",
        "### 1.2 Expressions régulières et segmentation\n",
        "\n",
        "Dans l'exemple précédent, on peut améliorer les choses en travaillant avec des $\\textbf{expressions régulières}$. Il s'agit alors de se donner des règles générales d'analyse permettant d'isoler ou même effacer certains caractères dans des environnements précis.\n",
        "\n",
        "-> https://docs.python.org/fr/3/library/re.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3tBPMG47mGt1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "tokens=re.split(r'[-\\s.,;!?]',sentence)\n",
        "print(tokens)\n",
        "#pour enlever les symboles . et ''\n",
        "tokens_cleared=[x for x in tokens if x and x not in '- \\t\\n.,;!?']\n",
        "print(tokens_cleared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeAeEMiimGt1"
      },
      "source": [
        "### 1.3 Tokenizers de référence\n",
        "\n",
        "Un grand nombre de librairies python centrées sur le traitement du langage naturel contiennent des implémentations de Tokenizer beaucoup plus sophistiqués que les méthodes décrites ci-dessus.\n",
        "\n",
        "-> https://spacy.io\n",
        "\n",
        "-> https://stanfordnlp.github.io/CoreNLP/, https://corenlp.run\n",
        "\n",
        "-> https://www.nltk.org\n",
        "\n",
        "On peut par exemple utiliser le Treebank Word Tokenizer de la dernière librairie pour segmenter notre phrase de travail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CmNpd0kDmGt1"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer,punkt\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokens=tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njeee6RtmGt2"
      },
      "source": [
        "## 2. Normalisation du vocabulaire\n",
        "\n",
        "Par normalisation du vocabulaire, on entend la gestion des modifications suivantes : prise en compte des majuscules, identification des racines apparentées, lemmatisation. La prise en compte de ces aspects permet d'affiner la représentation des textes et documents.\n",
        "\n",
        "### 2.1 Gestion des majuscules\n",
        "\n",
        "C'est le degré zéro de la normalisation. Dans la plupart des cas, il n'est pas nécessaire de distinguer les mots 'Phénomène' et 'phénomène' qui ne diffèrent que par la présence d'une majuscule.  On oublie facilement ces différences via la méthode $\\texttt{lower}$ de la classe $\\texttt{str}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dxYsCUT6mGt2"
      },
      "outputs": [],
      "source": [
        "print(tokens)\n",
        "tokens=[x.lower() for x in tokens]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCUbpc7ZmGt2"
      },
      "source": [
        "### 2.2 Prise en compte des \"stop words\"\n",
        "\n",
        "Les $\\textbf{stop words}$ sont les mots outils/liaisons d'une langue dont on peut en général considérer qu'ils véhiculent peu d'information sémantique. Il s'agit des articles, de certaines particules, de certaines interjections. On est donc tenté de les faire disparaître de la liste des jetons caractéristiques de notre phrase / document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AT_Lx6mmGt2"
      },
      "outputs": [],
      "source": [
        "stop_words = ['the','and','a','an','this','that','of']\n",
        "#tokens=tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "tokens = [x for x in tokens if x not in stop_words]\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHMtR1gHmGt2"
      },
      "source": [
        "Mais on peut utiliser des listes pré-construites associées aux librairies mentionnées.\n",
        "\n",
        "Voici la liste fournie par NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAc8D8H4mGt3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words=nltk.corpus.stopwords.words('german')\n",
        "\n",
        "print(len(stop_words))\n",
        "stop_words[:10]\n",
        "\n",
        "Satz=''' Kennst du das Land, wo die Zitronen bluhn '''\n",
        "Zeichen=tokenizer.tokenize(Satz)\n",
        "\n",
        "Zeichen = [x for x in Zeichen if x not in stop_words]\n",
        "print(Zeichen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkFf-CKCmGt3"
      },
      "source": [
        "Le module $\\texttt{sklearn.feature_extraction}$ de Scikitlearn possède également sa propre liste de stop words.  \n",
        "\n",
        "-> https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "\n",
        "Toutes ces listes ne sont pas totalement identiques et sont mises à jour régulièrement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkVJ_15xmGt3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
        "len(sklearn_stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDdr7PAfmGt3"
      },
      "source": [
        "Attention ! Dans certains cas, les majuscules sont porteuses de sens !\n",
        "\n",
        "### 2.3 Racinisation ou \"stemming\"\n",
        "\n",
        "Il s'agit de ne pas prendre en compte les variations d'écriture induites par : les marques du pluriel, les marques possessives et les déclinaisons. C'est une tâche bien plus délicate qu'il n'y paraît. Elle a pour objectif l'identification de jetons du vocabulaire à partir de critère morphologique.\n",
        "\n",
        "On peut bien sûr procéder de manière directe. Voici par exemple une fonction qui élimine tous les 's' en position finale des mots. Cette racinisation manuelle repose entièrement sur la manipulation d'expression régulière."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5bNYHPwmGt3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def stem_basic(phrase):\n",
        "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
        "\n",
        "word_test=stem_basic(\"nombres\")\n",
        "\n",
        "print(word_test)\n",
        "\n",
        "phrase_test =\"Les heures pourpres des nombres gris\"\n",
        "phrase_test_stemmed = stem_basic(phrase_test)\n",
        "\n",
        "print(phrase_test_stemmed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtURdFKTmGt3"
      },
      "source": [
        "Il existe bien entendu des stemmers très sophistiqués $\\textbf{adaptés aux spécificités de chaque langue}$.\n",
        "\n",
        "Pour l'anglais, les algorithmes de racinisation les plus utilisés sont Snowball et Porter.\n",
        "\n",
        "Voir https://tartarus.org/martin/PorterStemmer/, pour le dernier.\n",
        "\n",
        "Voir également https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py pour une implémentation 100% python de l'algorithme original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSf3CcjSmGt3"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ps=PorterStemmer()\n",
        "snow=SnowballStemmer(language='english')\n",
        "\n",
        "stop_words=nltk.corpus.stopwords.words('english')\n",
        "\n",
        "print(tokens)\n",
        "tokens_ps = [ps.stem(x) for x in tokens if x not in stop_words]\n",
        "tokens_bis = [snow.stem(x) for x in tokens if x not in stop_words]\n",
        "print(tokens_ps)\n",
        "print(tokens_bis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAvIa5VKmGt3"
      },
      "source": [
        "### 2.4 Lemmatisation\n",
        "\n",
        "La lemmatisation permet, comme la racinisation, d'identifier certains jetons. A la différence de cette dernière, elle s'appuie sur des informations d'ordre $\\textbf{sémantique}$.\n",
        "\n",
        "Elle s'aide également d'informations grammaticales telles que la $\\textbf{POS (Part of Speech)}$; qui correspond par exemple au deuxième argument de la fonction $\\texttt{lemmatize}$ implémentée dans NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDJlQY1UmGt3"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"manned\", pos=\"v\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"manned\", pos=\"a\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"calls\", pos=\"v\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"called\", pos=\"v\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdijRWy2mGt4"
      },
      "source": [
        "Si l'on dispose donc d'un étiquetage de la phrase (document) en terme de POS, on peut appliquer le lemmatizer à chaque jeton en tenant compte de cette information supplémentaire.\n",
        "\n",
        "Pour plus d'information, on pourra par exemple consulter :\n",
        "\n",
        "-> https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4I8ylJsmGt4"
      },
      "source": [
        "## 3. D'autres jetons : les n-grams\n",
        "\n",
        "Jusqu'à présent (et ce sera globalement le cas dans ce notebook), les jetons ont été implicitement pensés comme des mots. Mais les mots ne sont pas les seules entités porteuses de sens. Il arrive que des suites de plusieurs mots (ou jetons après tokenization) correspondent à une unité sémantique. Il est alors logique d'associer un seul jeton à une suite de plusieurs mots.\n",
        "\n",
        "Dans le cadre de la segmentation, ces suites de $n$ mots sont appelées $\\textbf{n-grams}$. La librairie NLTK dispose de fonctions permettant de les intégrer à la segmentation. Un exemple ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiStKUKEmGt4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def tokenize_ngrams(text, n ):\n",
        "    n_grams = ngrams(word_tokenize(text), n)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "\n",
        "tokens_2grams=tokenize_ngrams(sentence,2)\n",
        "tokens_3grams=tokenize_ngrams(sentence,3)\n",
        "print(tokens_2grams)\n",
        "print(tokens_3grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq5kwdr5mGt4"
      },
      "source": [
        "## 4. Représentations vectorielles (première partie)\n",
        "\n",
        "### 4.1 Représentation vectorielle \"one-hot-vector\"\n",
        "\n",
        "On va s'appuyer sur la segmentation précédente afin de créer une représentation vectorielle élémentaire de notre phrase de référence. Une représentation \"one-hot-vector\" repose sur la donnée du vocabulaire à partir duquel a été composé le document.\n",
        "\n",
        "A chaque jeton du document, on associe un vecteur qui contient uniquement des zéros, à l'exception de la coordonnée du mot du vocabulaire auquel il correspond."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BTgTAru7mGt4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "voc = sorted(set(tokens))\n",
        "print(voc)\n",
        "','.join(voc)\n",
        "n_tokens = len(tokens)\n",
        "n_voc = len(voc)\n",
        "\n",
        "OHV = np.zeros((n_tokens,n_voc),int)\n",
        "\n",
        "for i,word in enumerate(tokens):\n",
        "    OHV[i,voc.index(word)]=1\n",
        "\n",
        "#' '.join(voc)\n",
        "print(OHV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLoxo3KjmGt4"
      },
      "source": [
        "Afin de rendre cette représentation plus lisible, on peut utiliser pandas. Ceci permet notamment d'étiqueter les colonnes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtQUB0LcmGt4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.DataFrame(OHV, columns=voc)\n",
        "df[df==0]=''\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T60MqYB9mGt4"
      },
      "source": [
        "Il s'agit d'une première manière d'associer une représentation vectorielle \"numérique\" à une chaine de caractère. Cette méthode est simple mais gourmande en terme de mémoire.\n",
        "\n",
        "A noter : on rencontrera à nouveau ce type de représentation dans le cadre de la construction des word embedding (Word2vec en particuluer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXm-kJh6mGt4"
      },
      "source": [
        "### 4.2 Représentation vectorielle d'un corpus : Bag-of-words\n",
        "\n",
        "Supposons à présent que l'on travaille non plus avec un document mais avec $\\textbf{un corpus de documents}$ (c'est à dire un ensemble de documents). Le code ci-dessous propose une réprésentation spécifique du corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWbYY6RnmGt4"
      },
      "outputs": [],
      "source": [
        "sentences=sentence\n",
        "sentences+='''Socrates was also a famous philosopher who taught Plato in Athens. \\n '''\n",
        "sentences+='''Socrates compares the creation of words to the work of an artist. \\n '''\n",
        "sentences+='''The creation of words uses letters containing certain sounds to express the essence of words subject. \\n '''\n",
        "sentences+='''The famous Hermogenes opposing Socrates suggests that words do not express the essence of their subject'''\n",
        "\n",
        "print(sentences)\n",
        "corpus={}\n",
        "for i,sent in enumerate(sentences.split('\\n')):\n",
        "    #Tokenization à l'échelle de la phrase avec racinisation et nettoyage des stop words\n",
        "    tokens=tokenizer.tokenize(sent)\n",
        "    tokens=[ps.stem(x) for x in tokens if x not in stop_words]\n",
        "    corpus['sent{}'.format(i)]=dict((tok,1) for tok in tokens)\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "# Création d'un dataframe à partir du corpus (liste de vecteurs)\n",
        "df=pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
        "\n",
        "#df[df.columns[20:]]\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg5GmSb_mGt4"
      },
      "source": [
        "### 4.3 Mesure de similarité\n",
        "\n",
        "A partir d'une telle représentation vectorielle, il devient possible de comparer les phrases (resp. textes) d'un texte (resp. corpus) entre elles. On utilise à cet effet le produit scalaire aux vecteurs BOW associés aux différentes phrases du texte.\n",
        "\n",
        "$$ \\langle senti, sentj \\rangle = \\sum_{k=1}^{n_{\\mathrm{voc}}} senti[k] sentj[k]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hoxA9Q4mGt4"
      },
      "outputs": [],
      "source": [
        "## Statistiques sur df via pandas\n",
        "\n",
        "tab=df.T\n",
        "print(df.describe())\n",
        "print(df.cov())\n",
        "\n",
        "\n",
        "## Calculs de similarité\n",
        "\n",
        "print(tab.sent0.dot(tab.sent1))\n",
        "print(tab.sent1.dot(tab.sent2))\n",
        "print(tab.sent0.dot(tab.sent3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cJB3KRkmGt5"
      },
      "source": [
        "# 5. Analyse de sentiment\n",
        "\n",
        "L'analyse de sentiment est l'une des tâches les plus répandues en NLP. Si l'on peut s'y atteler à l'aide d'outils sophistiqués (réseaux de neurones et autres), souvent nécessaires quand la masse et la complexité des données augmentent. Pour des corpus restreints et des données particulières, on peut parvenir à de bons résultats via des moyens relativement \"simples\".\n",
        "\n",
        "## 5.1 Un premier exemple avec VADER\n",
        "\n",
        "L'algorithme VADER est implémenté dans NLTK mais également dans le package vaderSentiment maintenu par l'auteur de l'algorithme. Il est particulièrement adapté à l'analyse de sentiments pour des textes courts issus des réseaux sociaux ou SMS.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKzoiC0wmGt5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install vaderSentiment\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sa=SentimentIntensityAnalyzer()\n",
        "sa.lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHAWNNGhmGt5"
      },
      "source": [
        "L'algorithme associe des scores aux différents messages selon trois items : positivity, negativity et neutral. Il calcule ensuite un score composé à partir de ces trois champs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9R8L5DrmGt5"
      },
      "outputs": [],
      "source": [
        "sa.polarity_scores(text=\"Aristotle is so amazing. His books are just perfect ;-)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y72N4o3-mGt5"
      },
      "outputs": [],
      "source": [
        "corpus=[\"Awesome ! You are the best\", \"So dumb ! You are useless\", \"It was so so, well written for sure but boring as well\"]\n",
        "\n",
        "for text in corpus:\n",
        "    scores=sa.polarity_scores(text)\n",
        "    print('{:+}: {}'.format(scores['compound'],text))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7uFvL5XmGt5"
      },
      "source": [
        "## 5.2 Classification bayésienne naïve\n",
        "\n",
        "On commence par récupérer une base de données qui contient des critiques de films auxquelles sont associées des notes. (tirée de Hutto et Gilbert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS5uVrBmmGt5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nlpia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANI_AIklmGt7"
      },
      "outputs": [],
      "source": [
        "from nlpia.data.loaders import get_data\n",
        "movies=get_data('hutto_movies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvQghQkBmGt7"
      },
      "outputs": [],
      "source": [
        "type(movies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ-78cedmGt7"
      },
      "source": [
        "Un coup d'oeil rapide au data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Awg7ahEmGt8"
      },
      "outputs": [],
      "source": [
        "movies.head().round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jARduNB0mGt8"
      },
      "outputs": [],
      "source": [
        "movies.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dQvAcBTmGt8"
      },
      "source": [
        "### Segmentation et création d'un BOW à partir de movies\n",
        "\n",
        "On va à présent appliquer le tokenizer et créer un vecteur BOW pour chaque critique de films, exactement comme dans le cadre de la section 4.2. Le tout est emballé dans un DataFrame pandas pour plus de lisibilité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY8B0Cn6mGt8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.width',75)\n",
        "\n",
        "from nltk.tokenize import casual_tokenize # tokenizer adapté à ce type de contenu\n",
        "from collections import Counter\n",
        "\n",
        "BOWS=[]\n",
        "for text in movies.text:\n",
        "    BOWS.append(Counter(casual_tokenize(text)))\n",
        "\n",
        "#print(BOWS)\n",
        "df_BOWS=pd.DataFrame.from_records(BOWS) ## la fonction from_records se charge d'identifier les termes\n",
        "df_BOWS.head()\n",
        "\n",
        "df_BOWS=df_BOWS.fillna(0).astype(int)\n",
        "print(df_BOWS.shape)\n",
        "\n",
        "df_BOWS.head()\n",
        "#df_BOWS.head()[list(BOWS[0].keys)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbKUzppymGt8"
      },
      "outputs": [],
      "source": [
        "df_BOWS.head()[list(BOWS[6].keys())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHD1E2mUmGt8"
      },
      "source": [
        "On va maintenant appliquer le $\\textbf{classificateur bayésien naïf}$ de scikit-learn afin de résoudre le problème de classification.\n",
        "\n",
        "-> https://scikit-learn.org/stable/modules/naive_bayes.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqD_C9M6mGt8"
      },
      "outputs": [],
      "source": [
        "print(movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYnk7Y7gmGt8"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Préparation de l'entrainement\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_BOWS, movies.sentiment > 0, test_size=0.2, random_state=42)\n",
        "\n",
        "print(y_train)\n",
        "## Création du classificateur\n",
        "NBC=MultinomialNB()\n",
        "#NBC=NBC.fit(X_train, movies.sentiment > 0) # La classe doit avoir un type int ou bool\n",
        "NBC=NBC.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Homogeneisation des notes vs proba\n",
        "pred_train_cat=NBC.predict_proba(X_train)\n",
        "print(\"pred_train_cat\",pred_train_cat)\n",
        "test_train_cat=pred_train_cat[:,0]<pred_train_cat[:,1]\n",
        "error_train_cat=abs(test_train_cat.T!=y_train).mean()\n",
        "print(\"error_train\",error_train_cat)\n",
        "#error.mean()\n",
        "\n",
        "pred_test_cat=NBC.predict_proba(X_test)\n",
        "print(\"pred_train_cat\",pred_test_cat)\n",
        "test_test_cat=pred_test_cat[:,0]<pred_test_cat[:,1]\n",
        "error_test_cat=abs(test_test_cat.T!=y_test).mean()\n",
        "print(\"error_test\",error_test_cat)\n",
        "#error.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUvLgcKYmGt8"
      },
      "source": [
        "L'efficacité demeure-t-elle lorsqu'on change de données ? On va tester l'approche précédente sur un data set un peu différent qui contient des critiques de produits et non plus de films"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr4CWpa7mGt8"
      },
      "outputs": [],
      "source": [
        "...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N6XdFVvmGt8"
      },
      "source": [
        "Le résultat n'est pas aussi satisfaisant que précédemment.\n",
        "\n",
        "Pourquoi ? Quelles pistes d'amélioration ?"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Format de la Cellule Texte Brut",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}